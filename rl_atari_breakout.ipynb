{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdEmjyZtQTwG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = 4_000              # Number of episodes to run for training \n",
    "MEMORY_BUFFER = 100_000         # Experience memory size  \n",
    "BATCH_SIZE = 64                 # Batch size for neural network training \n",
    "UPDATE_STEP = 4                 # Number of steps to update target model\n",
    "\n",
    "TAU =  0.05                    # Parameter for the soft update of the weights \n",
    "GAMMA = 0.995                    # Discount factor for the calculation of the return \n",
    "\n",
    "N_ACTIONS = 4              # Number of possible actions  (0: NOOP, 1: FIRE, 2: RIGHT, 3: LEFT)\n",
    "\n",
    "EPSILON_INTERVALS = [(600, .9), (800, .8), (1_200, .7), (1_400, .6), (1_800, .5), (2_200, .4), (2_600, .3), (3_000, .2), (3_400, .1), (3_800, .05), (4_000, .025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(episode):\n",
    "\n",
    "    for l, e in EPSILON_INTERVALS:\n",
    "        if episode <= l:\n",
    "            return e\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture:\n",
    "#   Not taken from: https://arxiv.org/pdf/1312.5602 \n",
    "#   Using https://keras.io/examples/rl/deep_q_network_breakout/ instead\n",
    "#\n",
    "#   TODO: review this description, corresponds to the Deepmind architecture\n",
    "#   Input: (4, 84, 84)\n",
    "#   Convolution: valid, f=8, s=4, 16 filters -> (16, 20, 20)\n",
    "#   Convolution: valid, f=4, s=2, 32 filters -> (32, 9, 9)\n",
    "#   Flatten: \n",
    "#   Dense: -> 256\n",
    "#   Dense: -> 4\n",
    "\n",
    "def q_model():\n",
    "\n",
    "    return keras.Sequential ([\n",
    "            keras.layers.Input(shape=(4, 84, 84)),\n",
    "            keras.layers.Conv2D(32,(8, 8), data_format='channels_first', strides=4, activation='relu'),\n",
    "            keras.layers.Conv2D(64,(4, 4), data_format='channels_first', strides=2, activation='relu'),\n",
    "            keras.layers.Conv2D(64,(3, 3), data_format='channels_first', strides=2, activation='relu'),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(512, activation='relu'),\n",
    "            keras.layers.Dense(4)\n",
    "        ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(memory, sample_size):\n",
    "\n",
    "  samples = random.sample(memory, sample_size)\n",
    "  \n",
    "  states, actions, states_next, rewards, terminated, truncated = zip(*samples)\n",
    "\n",
    "  states = np.stack(states, axis=0)\n",
    "  states_next = np.stack(states_next, axis=0)\n",
    "  actions = np.array(actions, dtype='int32')\n",
    "  rewards = np.array(rewards, dtype='float32')\n",
    "  terminated = np.array(terminated, dtype='float32')\n",
    "\n",
    "  return states, actions, states_next, rewards, terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train step\n",
    "\n",
    "@tf.function\n",
    "def train_step(samples, q_function, q_function_target, loss_function, optimizer):\n",
    "\n",
    "  states, actions, states_next, rewards, terminated = samples\n",
    "\n",
    "  # Use q_function_target to calculate Q(state_prime, action) used in Bellman equation\n",
    "  max_qsa = tf.math.reduce_max(q_function_target(states_next, training=False), axis=1)\n",
    "\n",
    "  # Bellman equation\n",
    "  # For terminated states, q_value is just the reward\n",
    "  qvalues_target = (rewards + (1 - terminated) * GAMMA * max_qsa)\n",
    "\n",
    "  # Calculate mask so loss is calculated only on the action taken\n",
    "  mask = tf.one_hot(actions, N_ACTIONS) \n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run model\n",
    "    qvalues_predictions = tf.reduce_max(\n",
    "      tf.multiply(q_function(states, training=True), mask), axis=1)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_function(qvalues_target, qvalues_predictions)\n",
    "\n",
    "  # Back propagation\n",
    "  grads = tape.gradient(loss, q_function.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, q_function.trainable_variables))\n",
    "\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Taken from: https://keras.io/examples/keras_recipes/reproducibility_recipes/\n",
    "keras.utils.set_random_seed(812)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng for the environment and to reshuffle samples\n",
    "rng = np.random.default_rng(seed=34)\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\", frameskip=1)\n",
    "\n",
    "# The AtariPreprocessing wrapper implements among others the following transformations,\n",
    "#  - Frame skip (4)\n",
    "#  - Resize to square image (resizes from the original 210x180 to 84x84)\n",
    "#  - Grey scale\n",
    "env = gym.wrappers.AtariPreprocessing(env)\n",
    "\n",
    "# The game won't stop until the Fire action is taken, so the episode could run for ever.\n",
    "# This wrapper signals truncated after the given timesteps, giving the agent the opportunity to \n",
    "# learn that.\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=1000)\n",
    "\n",
    "# The FrameStack wrapper stacks n observations (4) in a rolling manner\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "env.np_random = rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model used to select actions. Gets updated every batch\n",
    "q_function = q_model()\n",
    "\n",
    "# Model used to calculate the rewards for the future predictions that are used\n",
    "# in the Bellman equation used as target for the updates of q_function. It is a different\n",
    "# one for stability. Gets updated every UPDATE_STEP\n",
    "q_function_target = q_model()\n",
    "q_function_target.set_weights(q_function.get_weights())\n",
    "\n",
    "loss_function = tf.keras.losses.Huber()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_function.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = deque(maxlen=MEMORY_BUFFER) \n",
    "loss_hist = []\n",
    "reward_hist = []\n",
    "duration_hist = []\n",
    "step = 0\n",
    "episode_reward = 0      # To accumulate the rewards of the episode\n",
    "episode_duration = 0    # To keep a step count for the episode\n",
    "\n",
    "for episode in trange(N_EPISODES, desc=\"Episode\"):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    epsilon = get_epsilon(episode)\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        # take action using e-greedy selection\n",
    "        if rng.choice([True, False], p=[epsilon, 1 - epsilon]): \n",
    "            action = rng.choice(N_ACTIONS)\n",
    "        else:\n",
    "            # Use Q function to select next action\n",
    "            state_input = np.expand_dims(state, axis=0)\n",
    "            qvalue_predictions = q_function(state_input, training=False)\n",
    "\n",
    "            action = np.argmax(qvalue_predictions[0])\n",
    "\n",
    "        state_next, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        memory.append((state, action, state_next, reward, terminated, truncated))\n",
    "\n",
    "        update = (step + 1 >= BATCH_SIZE) and not ((step + 1) % UPDATE_STEP)\n",
    "        if update:\n",
    "            #\n",
    "            # Update q-function\n",
    "            #\n",
    "\n",
    "            samples = get_samples(memory, BATCH_SIZE)\n",
    "\n",
    "            # Train network\n",
    "            loss = train_step(samples, q_function, q_function_target, loss_function, optimizer)\n",
    "            \n",
    "            loss_hist.append(loss.numpy())\n",
    "\n",
    "            # Update target model\n",
    "            new_weights = []\n",
    "            for new_w, old_w in zip(q_function.get_weights(), q_function_target.get_weights()):\n",
    "                new_weights.append(TAU * new_w + (1 - TAU) * old_w)\n",
    "\n",
    "            q_function_target.set_weights(new_weights)\n",
    "\n",
    "        step += 1\n",
    "        episode_duration += 1\n",
    "        episode_reward += reward\n",
    "        if terminated or truncated:\n",
    "            # Experiment ended, reset the environment\n",
    "            reward_hist.append(episode_reward)\n",
    "            duration_hist.append(episode_duration)\n",
    "            episode_reward = 0\n",
    "            episode_duration = 0\n",
    "        else:\n",
    "            state = state_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.Series(loss_hist)\n",
    "loss_df.rolling(window=20).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_episodes = pd.DataFrame({\n",
    "    \"Reward\": reward_hist,\n",
    "    \"Duration\": duration_hist\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.saving.save_model(q_function, \"q_function.keras\")\n",
    "df_episodes.to_pickle(\"./df_episodes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_episodes = pd.read_pickle('./df_episodes.pkl')\n",
    "\n",
    "df_episodes.rolling(window=50).mean().plot(subplots=True, layout=(1,2), figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation\n",
    "q_function = keras.saving.load_model(\"q_function.keras\")\n",
    "\n",
    "rng = np.random.default_rng(seed=34)\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\", frameskip=1)\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "env = gym.wrappers.AtariPreprocessing(env)\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "cumm_reward = 0\n",
    "n = 10\n",
    "\n",
    "for i in trange(n):\n",
    "    state, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not (terminated or truncated):\n",
    "        if rng.choice([True, False], p=[0.01, 0.99]): \n",
    "            # The game won't start until fire is pressed. This is to ensure \n",
    "            # it does (it may not have been learnt)\n",
    "            action = 1\n",
    "        else:\n",
    "            state_input = np.expand_dims(state, axis=0)\n",
    "            \n",
    "            state_predictions = q_function(state_input, training=False)\n",
    "            \n",
    "            # take action using e-greedy selection    \n",
    "            action = np.argmax(state_predictions[0])\n",
    "        \n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        cumm_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print (f\"Avg reward per episode {cumm_reward/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_function = keras.saving.load_model(\"q_function.keras\")\n",
    "\n",
    "rng = np.random.default_rng(seed=34)\n",
    "\n",
    "#env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "env = gym.make(\"ALE/Breakout-v5\", frameskip=1, render_mode='rgb_array')\n",
    "env = gym.wrappers.AtariPreprocessing(env)\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "env = gym.wrappers.RecordVideo(env, \"videos_atari\", episode_trigger= lambda e: True)\n",
    "\n",
    "# env.metadata['render_fps'] = 24\n",
    "\n",
    "episodes_rewards = []\n",
    "\n",
    "for i in trange(10):\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    this_reward = []\n",
    "    \n",
    "    while True:\n",
    "        if rng.choice([True, False], p=[0.01, 0.99]): \n",
    "            # The game won't start until fire is pressed. This is to ensure \n",
    "            # it does (it may not have been learnt)\n",
    "            action = 1\n",
    "        else:\n",
    "            state_input = np.expand_dims(state, axis=0)\n",
    "            state_predictions = q_function(state_input, training=False)\n",
    "            \n",
    "            action = np.argmax(state_predictions[0])\n",
    "        \n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "        this_reward.append(reward)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            episodes_rewards.append(this_reward)\n",
    "\n",
    "            break\n",
    "    \n",
    "df_rewards = pd.DataFrame(episodes_rewards).transpose()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_rewards.cumsum().plot(subplots=True, layout=(2,5), figsize=(15,5), sharey=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
