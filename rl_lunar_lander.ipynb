{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OulqNGYOQTwE"
   },
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "## Implementation with a neural network\n",
    "\n",
    "Idea: Train a neural network to calculate, for a given state $s$, the return of the state action value functions for the actions possible in that state, so we can choose the one with largest $Q(s, a)$. In other words, train a neural network that given s and a returns $y \\approx Q(a,a)$. Or, in less words, train the neural network to learn the Bellman equation.\n",
    "\n",
    "To do so, we can create a large set of tuples\n",
    "\n",
    "$$(s^{(1)}, a^{(1)}, R(s^{(1)}), s'^{(1)}) \\\\ (s^{(2)}, a^{(2)}, R(s^{(2)}), s'^{(2)}) \\\\ \\dots$$\n",
    "\n",
    "And then, the training examples for the neural network will be:\n",
    "\n",
    "* For the inputs $x$, each of the tuples \n",
    "$$(s^{(1)}, a^{(1)}), (s^{(2)}, a^{(2)}), \\dots$$\n",
    "* For the target values y, the corresponding \n",
    "$$Q(s^{(1)},a^{(1)}), Q(s^{(2)},a^{(2)}), \\dots$$\n",
    "\n",
    "calculated with the Bellman equation, for example\n",
    "\n",
    "$$Q(s^{(1)}, a^{(1)}) = R(s^{(1)}) + \\gamma \\max_{a'} Q(s'^{(1)}, a')$$\n",
    "\n",
    "Note that the target values $y$ depend only on the last two elements of the tuples $(s^{(i)}, a^{(i)}, R(s^{(i)}), s'^{(i)})$\n",
    "\n",
    "At the begining, we don't know the $Q(s, a)$ function, but it can be initialized randomly. In every step, it will get better.\n",
    "\n",
    "Learning algorithm (sometimes call the **Deep-Q network**)\n",
    "\n",
    "<pre>\n",
    "    Initialize Q_nn neural network randomly as guess of Q(s, a)\n",
    "    Repeat {\n",
    "        Take actions to generate tuples (s, a, R(s), s')\n",
    "        Store the 10,000 more recent examples of these tuples (replay buffer)\n",
    "        Train neural network:\n",
    "            Create training set of 10,000 examples x, y using\n",
    "                x = (s, a) and y = R(s) + &gamma; max<sub>a'</sub> Q_nn(s', a')\n",
    "            Train Q<sub>new</sub> such that Q<sub>new</sub>(s, a) &asymp; y\n",
    "        Set Q_nn = Q<sub>new</sub>\n",
    "    }\n",
    "</pre>\n",
    "\n",
    "One possible architecture of the neural network is (from course example for lunar lander, with 8 parameters for the state, and 4 possible actions, one hot encoded):\n",
    "\n",
    "```Pyhton\n",
    "tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "]) \n",
    "```\n",
    "\n",
    "An improved architecture uses (for this case) four units in the output layer, to compute at the same time the $Q(s, a)$ function for all the possible actions in one state. The input, in this case, is the 8 parameters that represent the state.\n",
    "\n",
    "```Pyhton\n",
    "tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(4)\n",
    "]) \n",
    "```\n",
    "\n",
    "> Note: With this network configuration, the standard way to calculate the loss won't work, as the Bellman equation uses the four outputs. See how it is done in the implementation example bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdEmjyZtQTwG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_DIM = 8                   # Dimension of the state array\n",
    "N_ACTIONS = 4                   # Number of possible actions\n",
    "\n",
    "N_EPISODES = 1_000              # Number of episodes to run for training \n",
    "MEMORY_BUFFER = 100_000         # Experience memory size  \n",
    "BATCH_SIZE = 64                 # Batch size for neural network training \n",
    "UPDATE_STEP = 4                 # Number of steps to update target model\n",
    "\n",
    "TAU =  0.05                    # Parameter for the soft update of the weights \n",
    "GAMMA = 0.995                    # Discount factor for the calculation of the return \n",
    "EPSILON_INTERVALS = [(200, .9), (400, .5), (600, .3), (1000, .1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(episode):\n",
    "\n",
    "    for l, e in EPSILON_INTERVALS:\n",
    "        if episode <= l:\n",
    "            return e\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_model():\n",
    "\n",
    "    return keras.models.Sequential ([\n",
    "            keras.layers.Input(shape=(STATE_DIM,)),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dense(4)\n",
    "        ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(memory, sample_size):\n",
    "\n",
    "  samples = random.sample(memory, sample_size)\n",
    "  \n",
    "  states, actions, states_next, rewards, terminated, truncated = zip(*samples)\n",
    "\n",
    "  states = tf.convert_to_tensor(states, dtype='float32')\n",
    "  actions = np.array(actions, dtype='int32')\n",
    "  states_next = tf.convert_to_tensor(states_next, dtype='float32')\n",
    "  rewards = np.array(rewards, dtype='float32')\n",
    "  terminated = np.array(terminated, dtype='float32')\n",
    "\n",
    "  return states, actions, states_next, rewards, terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train step\n",
    "\n",
    "@tf.function\n",
    "def train_step(samples, q_function, q_function_target, loss_function, optimizer):\n",
    "\n",
    "  states, actions, states_next, rewards, terminated = samples\n",
    "\n",
    "  # Use q_function_target to calculate Q(state_prime, action) used in Bellman equation\n",
    "  max_qsa = tf.math.reduce_max(q_function_target(states_next, training=False), axis=1)\n",
    "\n",
    "  # Bellman equation\n",
    "  # For terminated states, q_value is just the reward\n",
    "  qvalues_target = (rewards + (1 - terminated) * GAMMA * max_qsa)\n",
    "\n",
    "  # Calculate mask so loss is calculated only on the action taken\n",
    "  mask = tf.one_hot(actions, N_ACTIONS) \n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run model\n",
    "    qvalues_predictions = tf.reduce_max(\n",
    "      tf.multiply(q_function(states, training=True), mask), axis=1)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_function(qvalues_target, qvalues_predictions)\n",
    "\n",
    "  # Back propagation\n",
    "  grads = tape.gradient(loss, q_function.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, q_function.trainable_variables))\n",
    "\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Taken from: https://keras.io/examples/keras_recipes/reproducibility_recipes/\n",
    "keras.utils.set_random_seed(812)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng for the environment and to reshuffle samples\n",
    "rng = np.random.default_rng(seed=34)\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode=None)\n",
    "env.np_random = rng\n",
    "\n",
    "# See https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "# Actions: 0: do nothing, 1: fire left orientation engine, 2: fire main engine, 3: fire right orientation engine\n",
    "# State: [x, y, vx, vy, theta, omega, bool, bool ] The booleans represent if the legs are in contact with the ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model used to select actions. Gets updated every batch\n",
    "q_function = q_model()\n",
    "\n",
    "# Model used to calculate the rewards for the future predictions that are used\n",
    "# in the Bellman equation used as target for the updates of q_function. It is a different\n",
    "# one for stability. Gets updated every UPDATE_STEP\n",
    "q_function_target = q_model()\n",
    "q_function_target.set_weights(q_function.get_weights())\n",
    "\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = deque(maxlen=MEMORY_BUFFER) \n",
    "loss_hist = []\n",
    "reward_hist = []\n",
    "duration_hist = []\n",
    "step = 0\n",
    "episode_reward = 0      # To accumulate the rewards of the episode\n",
    "episode_duration = 0    # To keep a step count for the episode\n",
    "\n",
    "\n",
    "for episode in trange(N_EPISODES, desc=\"Episode\"):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_duration = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    epsilon = get_epsilon(episode)\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        # take action using e-greedy selection\n",
    "        if rng.choice([True, False], p=[epsilon, 1 - epsilon]): \n",
    "            action = rng.choice(N_ACTIONS)\n",
    "        else:\n",
    "            # Use Q function to select next action\n",
    "            state_tensor = tf.convert_to_tensor(state, dtype='float32')\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            qvalue_predictions = q_function(state_tensor, training=False)\n",
    "\n",
    "            action = np.argmax(qvalue_predictions[0])\n",
    "\n",
    "        state_next, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        memory.append((state, action, state_next, reward, terminated, truncated))\n",
    "\n",
    "        update = (step + 1 >= BATCH_SIZE) and not ((step + 1) % UPDATE_STEP)\n",
    "        if update:\n",
    "            # Update q-function\n",
    "\n",
    "            samples = get_samples(memory, BATCH_SIZE)\n",
    "\n",
    "            # Train network\n",
    "            loss = train_step(samples, q_function, q_function_target, loss_function, optimizer)\n",
    "            \n",
    "            loss_hist.append(loss)\n",
    "\n",
    "            # Update target model\n",
    "            new_weights = []\n",
    "            for new_w, old_w in zip(q_function.get_weights(), q_function_target.get_weights()):\n",
    "                new_weights.append(TAU * new_w + (1 - TAU) * old_w)\n",
    "\n",
    "            q_function_target.set_weights(new_weights)\n",
    "\n",
    "        step += 1\n",
    "        episode_duration += 1\n",
    "        episode_reward += reward\n",
    "        if terminated or truncated:\n",
    "            # Experiment ended, reset the environment\n",
    "            reward_hist.append(episode_reward)\n",
    "            duration_hist.append(episode_duration)\n",
    "        else:\n",
    "            state = state_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.Series(loss_hist)\n",
    "loss_df.rolling(window=500).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_episodes = pd.DataFrame({\n",
    "    \"Reward\": reward_hist,\n",
    "    \"Duration\": duration_hist\n",
    "    })\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "window=50\n",
    "\n",
    "color = 'tab:orange'\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward', color=color)\n",
    "ax1.plot(range(df_episodes.shape[0]), df_episodes[\"Reward\"].rolling(window=window).mean(), color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx() # Instantiate a second Axes that shares the same x-axis\n",
    "\n",
    "color='tab:blue'\n",
    "ax2.set_ylabel(\"Duration\", color=color)\n",
    "ax2.plot(range(df_episodes.shape[0]), df_episodes[\"Duration\"].rolling(window=window).mean(), color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.saving.save_model(q_function, \"q_function.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation\n",
    "demo_env = gym.make('LunarLander-v2', render_mode=None)\n",
    "count = 0\n",
    "landed = 0\n",
    "\n",
    "for i in range(100):\n",
    "    state, _ = demo_env.reset()\n",
    "    \n",
    "    cumm_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        state_tensor = tf.convert_to_tensor(state, dtype='float32')\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "        \n",
    "        state_predictions = q_function(state_tensor, training=False)\n",
    "        \n",
    "        # take action using e-greedy selection    \n",
    "        action = np.argmax(state_predictions[0])\n",
    "        \n",
    "        state, reward, terminated, truncated, _ = demo_env.step(action)\n",
    "\n",
    "        cumm_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            if cumm_reward >= 200:\n",
    "                count += 1\n",
    "            if reward == 100:\n",
    "                landed += 1\n",
    "            break\n",
    "    \n",
    "demo_env.close()\n",
    "\n",
    "print (f\"Successful {count}/100\")\n",
    "print (f\"Landed {landed}/100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demo_env = gym.make('LunarLander-v2', render_mode='rgb_array_list')\n",
    "episodes_rewards = []\n",
    "\n",
    "for i in range(10):\n",
    "    state, _ = demo_env.reset()\n",
    "    \n",
    "    this_reward = []\n",
    "    \n",
    "    while True:\n",
    "        state_tensor = tf.convert_to_tensor(state, dtype='float32')\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "        \n",
    "        state_predictions = q_function(state_tensor, training=False)\n",
    "        \n",
    "        # take action using e-greedy selection\n",
    "        \n",
    "        action = np.argmax(state_predictions[0])\n",
    "        \n",
    "        state, reward, terminated, truncated, _ = demo_env.step(action)\n",
    "                \n",
    "        this_reward.append(reward)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            save_video(\n",
    "                demo_env.render(),\n",
    "                \"videos\",\n",
    "                episode_trigger=(lambda a: True),\n",
    "                fps=demo_env.metadata[\"render_fps\"], \n",
    "                episode_index=i\n",
    "            )\n",
    "\n",
    "            episodes_rewards.append(this_reward)\n",
    "            break\n",
    "    \n",
    "df_rewards = pd.DataFrame(episodes_rewards).transpose()\n",
    "demo_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_rewards.cumsum().plot(subplots=True, layout=(2,5), figsize=(15,5), sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
